---
title: "Procesamiento de datos multivariados. Los peces y el mercurio"
author: "Yoceline Aralí Mata Ledezma A01562116"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#data =read.csv("mercurio.csv")
```

# Análisis de normalidad

## A. Realice la prueba de normalidad de Mardia y la prueba de Anderson Darling para identificar las variables que son normales y detectar posible normalidad multivariada de grupos de variables. 

```{r}
install.packages("mnormt")
install.packages("MVN")
```

```{r}
library(MVN)

data = read.csv("mercurio.csv")
M = subset(data, select = -c(X1, X2) )

# Vector de medias
X = colMeans(M)
#Matriz de covarianza
S = cov(M)

## Test de Multinomalidad: Método Sesgo y kurtosis de Mardia
mvn(M,subset = NULL, mvn = "mardia", covariance = FALSE,showOutliers = FALSE)
```


* Variables normales según la prueba de Anderson-Darling: X4 y X10

* Tomando en cuenta los resultados de la prueba de Mardia, no se encontró normalidad multivariada entre el grupo de variables analizado.

## B. Realiza la prueba de Mardia y Anderson Darling de las variables que sí tuvieron normalidad en los incisos anteriores. Interpreta los resultados obtenidos con base en ambas pruebas y en la interpretación del sesgo y la curtosis de cada una de ellas.

```{r}
Mnorm = data.frame(X4 = M$X4, X10 = M$X10)

p = 2 #indica que se trata de dos variables
# Vector de medias
X = colMeans(Mnorm)
#Matriz de covarianza
S = cov(Mnorm)
#Distancia de Mahalanobis
d2M =  mahalanobis(Mnorm,X,S)

#Multinormalidad Test gráfico Q-Q Plot
plot(qchisq(((1:nrow(Mnorm)) - 1/2)/nrow(Mnorm),df=p),sort( d2M ) )
abline(a=0, b=1,col="red")

## Test de Multinomalidad: Método Sesgo y kurtosis de Mardia
mvn(Mnorm,subset = NULL, mvn = "mardia", covariance = FALSE,showOutliers = FALSE)

```


* Tomando en cuenta los resultados de la prueba de Anderson-Darling, se sabe que las variables X4 y X10 son normales. 

* Al observar los resultados de la prueba de Mardia se sabe que en este grupo de variables (X2 y X4) hay normalidad multivariada.

## C. Haz la gráfica de contorno de la normal multivariada obtenida en el inciso B.

```{r}
library(mnormt)

x <- seq(3, 10, 0.1) 
y <- seq(-1, 3, 0.1)

mu <- c(mean(Mnorm$X4), mean(Mnorm$X10))
sigma = cov(Mnorm)

f     <- function(x, y) dmnorm(cbind(x, y), mu, sigma)
z     <- outer(x, y, f)
#create surface plot
persp(x, y, z, theta=-30, phi=25, expand=0.6, ticktype='detailed', col = "pink")
```

```{r}
contour(x, y, z, col = "blue", levels = c(0.01,0.03, 0.05, 0.07, 0.1))
```


## D. Detecta datos atípicos o influyentes en la normal multivariada encontrada en el inciso B (auxíliate de la distancia de Mahalanobis y del gráfico QQplot multivariado)

```{r}
distancias = mahalanobis(Mnorm, mu, sigma)

chi2 = qchisq(0.9973, 2)

cat("chi-cuadrada", chi2, "\n")

distancias
```

Teniendo el valor de chi-cuadrada para una probabilidad del 99.73% y con 2 grados de libertad, el cual es 11.829, se sabe que las observaciones que tengan un valor de distancia menor o igual a 11.829 son aquellas que están dentro del contorno de probabilidad estimado del 99.73%, es decir que no son datos atípicos. En este caso ninguna de las distancias es mayor a 11.829, por lo que no hay datos atípicos.

```{r}
plot(qchisq(((1:nrow(Mnorm)) - 1/2)/nrow(Mnorm),df=2),sort(distancias))
abline(a=0, b=1,col="red")

```

Observando la gráfica qqplot parece que las distancias se acomodan casi idealmente en la línea, a excepción de algunos puntos al final de la gráfica. 

# Análisis de componentes principales


## A. Justifique por qué es adecuado el uso de componentes principales para analizar la base (haz uso de la matriz de correlaciones)

```{r}
S = cov(M)
Mcor = cor(M)

Mcor
```


Debido que hay alta correlación entre varias de las variables, es necesario reducir la dimensionalidad utilizando componentes principales, ya que se quiere conservar la mayor cantidad de información posible.

## B. Realiza el análisis de componentes principales y justifica el número de componentes principales apropiados para reducir la dimensión de la base

```{r}
eigen1 = eigen(Mcor)
eigen1
```

```{r}
corr = sum(diag(Mcor))

varianza = c(rep(corr, 11))

lambdas = data.frame(eigen1[1])
proporcion = lambdas/varianza 
proporcion
```

```{r}
cumsum(proporcion)
```


El análisis de componentes principales se realizó con la matriz de correlación, ya que los datos no se encontraban escalados, lo cual de haber utilizado la varianza, habría ocasionado que ciertas variables tuvieran más peso debido unicamente a que tenían mayores valores y no por el comportamiento de los datos.


Tomando en cuenta la suma acumulada de la proporción de varianza explicada por los componentes, los componentes principales recomendados para reducir la dimensionalidad son los primeros 5, ya que con estos se explica el 93% de los datos y se reduce la dimensionalidad a la mitad.
    
## C. Representa en un gráfico los vectores asociados a las variables y las puntuaciones de las observaciones de las dos primeras componentes

```{r}
cpa <- prcomp(M, scale=TRUE)

biplot(x = cpa, scale = 0, cex =0.6, col = c("red", "blue"))
```

## D. Interprete los resultados. 


Al observar los vectores propios y la gráfica de los componentes principales 1 y 2, se observa que en el componente principal 1, no varía mucho el peso entre las variables, sin embargo las variables 8 y 10 tienen un peso más bajo que en las otras. Sin embargo, en el componente dos, las variables 8 y 10 son las que ganan más importancias, mientras que las demás variables mantienen un valor por debajo de |0.2|.


# Conclusión general


Los componentes que tienen una mayor varianza explicada podrían indicar cuales son las variables que tienen más influencia, dentro de los primeros 5 componentes principales (con los que se llega al 93% de varianza explicada), se identificaron las siguientes variables con más peso en estos:  número de peces estudiados en el lago (X8), indicador de la edad de los peces (X12), calcio (X5) y clorofila (X6).


